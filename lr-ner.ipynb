{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b7d261d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.2\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "import sklearn\n",
    "import pycrfsuite\n",
    "from collections import defaultdict\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nerdata import Token, Chunk, LabeledSentence, chunks_from_bio_tag_seq, read_data, print_evaluation, print_output\n",
    "\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9183bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training and development data.\n",
    "train = read_data('../data/eng.train')\n",
    "dev = read_data('../data/eng.testa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c1eaf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word2features(sent, i):\n",
    "    \n",
    "    features = defaultdict(int)\n",
    "    word = sent.tokens[i].word\n",
    "    stoplist = stopwords.words('english')\n",
    "    \n",
    "    features['bias'] = 1\n",
    "    features['word.lower-' + word.lower()] = 1\n",
    "    features['word.isupper'] = 1 if word.isupper() else 0\n",
    "    features['word.ispunct'] = 1 if word[0] in string.punctuation else 0\n",
    "    features['word.isnumber'] = 1 if word.isalpha() else 0\n",
    "    features['word.length-' + str(len(word))] = 1\n",
    "    features['word.isstopword'] = 1 if word[0] in stoplist else 0\n",
    "    \n",
    "    \n",
    "    if i > 0:\n",
    "        word1 = sent.tokens[i - 1].word\n",
    "        features['-1:word.lower-' + word1.lower()] = 1\n",
    "        features['-1:word.isupper'] = 1 if word1.isupper() else 0\n",
    "        features['BOS'] = 0\n",
    "    else:\n",
    "        features['BOS'] = 1\n",
    "        \n",
    "    if i < len(sent) - 1:\n",
    "        word1 = sent.tokens[i + 1].word\n",
    "        postag1 = sent.tokens[i + 1].pos\n",
    "        features['+1:word.lower-' + word1.lower()] = 1\n",
    "        features['+1:word.isupper'] = 1 if word1.isupper() else 0\n",
    "        features['EOS'] = 0\n",
    "    else:\n",
    "        features['EOS'] = 1\n",
    "                \n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]\n",
    "\n",
    "def sent2labels(sent):\n",
    "    return sent.bio_tags\n",
    "\n",
    "def sent2words(sent):\n",
    "    return [token.word for token in sent.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "62030684",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "for s in train:\n",
    "    for i in range(len(s)):\n",
    "        trainX.append(s.tokens[i].word)\n",
    "        \n",
    "trainY = []\n",
    "for s in train:\n",
    "    for tag in s.bio_tags:\n",
    "        trainY.append(tag)\n",
    "\n",
    "\n",
    "devX = []\n",
    "for s in dev:\n",
    "    for i in range(len(s)):\n",
    "        devX.append(s.tokens[i].word)\n",
    "        \n",
    "devY = []\n",
    "for s in dev:\n",
    "    for tag in s.bio_tags:\n",
    "        devY.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "996ff01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_feat = []\n",
    "for s in train:\n",
    "    for i in range(len(s)):\n",
    "        trainX_feat.append(word2features(s,i))\n",
    "\n",
    "        \n",
    "devX_feat = []\n",
    "for s in dev:\n",
    "    for i in range(len(s)):\n",
    "        devX_feat.append(word2features(s,i))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a25b14fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "35f2fd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vocab = create_vocab(trainX_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9601fe60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61434"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c737e029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "              \n",
    "    return new_examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5bf6cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "devX_ids = features_to_ids(devX_feat, feature_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2eca7277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.950\n"
     ]
    }
   ],
   "source": [
    "lr_model = LogisticRegression(penalty = 'l2', C = 1.0, solver = 'lbfgs', max_iter = 1000)\n",
    "lr_model.fit(trainX_ids, trainY)\n",
    "\n",
    "print('Accuracy: %.3f' % lr_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd71cb7",
   "metadata": {},
   "source": [
    "accuracy after using the logistic regression algorithm is more than the crf model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
