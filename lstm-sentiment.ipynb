{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification with RNNs (LSTMs) \n",
    "\n",
    "In this assignment you will experiment with training and evaluating sentiment classification models that use recurrent neural networks (RNNs) implemented in PyTorch. If you run the code locally on your computer, you will need to install the <a href=\"https://pytorch.org/\">PyTorch</a> package, using the instructions below (installation with <a href=\"https://www.anaconda.com/\">conda</a> is recomended):\n",
    "\n",
    "https://pytorch.org/get-started/locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name : Naimisha Churi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "## Local computer:\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of ll cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *lstm-sentiment.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *lstm-sentiment.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly.\n",
    "7. Submit **both** your PDF and notebook on Canvas. Make sure the PDF and notebook show the outputs of the training and evaluation procedures. Also upload the **output** on the test datasets.\n",
    "8. Verify your Canvas submission contains the correct files by downloading them after posting them on Canvas.\n",
    "\n",
    "## Educational cluster:\n",
    "\n",
    "1. Please make sure to have entered your name above.\n",
    "2. Run the Python code on the cluster, using the instructions at:\n",
    "https://webpages.charlotte.edu/rbunescu/courses/itcs4111/centaurus.pdf\n",
    "3. Look at the Slurm output file and make sure all your solutions are there, displayed correctly.\n",
    "4. Edit the Analysis section in the notebook file, and save it as a PDF. Alternatively, you can use a text editor to edit yoru Analysis, then export it as PDF.\n",
    "5. Submit the **Slurm output file**, the **Python source code** file lstm-sentiment.py, and the **analysis PDF** on Canvas. Also upload the **output** on the test datasets.\n",
    "6. Verify your Canvas submission contains the correct files by downloading them after posting them on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import *\n",
    "from sentiment_data import *\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import NamedTuple\n",
    "\n",
    "#why is it like this?\n",
    "class HyperParams(NamedTuple):\n",
    "    lstm_size: int\n",
    "    hidden_size: int\n",
    "    lstm_layers: int\n",
    "    drop_out: float\n",
    "    num_epochs: int\n",
    "    batch_size: int\n",
    "    seq_max_len: int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based training and evaluation procedures\n",
    "\n",
    "We will use the RNNet class defined in `models.py` that uses LSTMs implemented in PyTorch. Depending on the options, this class runs one LSTM (forward) or two LSTMS (bidirectional, forward-backward) on the padded input text. The last state (or concatenated last states), or the average of the states, is used as input to a fully connected network with 3 hidden layers, with a final output sigmoid node computing the probability of the positive class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training procedure for LSTM-based models\n",
    "def train_model(hp: HyperParams,\n",
    "                train_exs: List[SentimentExample],\n",
    "                dev_exs: List[SentimentExample],\n",
    "                test_exs: List[SentimentExample], \n",
    "                word_vectors: WordEmbeddings,\n",
    "                use_average, bidirectional):\n",
    "    train_size = len(train_exs)\n",
    "    class_num = 1\n",
    "    \n",
    "    # Specify training on gpu: set to False to train on cpu\n",
    "    # use_gpu = False\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    if use_gpu: # Set tensor type when using GPU\n",
    "        float_type = torch.cuda.FloatTensor\n",
    "    else: # Set tensor type when using CPU\n",
    "        float_type = torch.FloatTensor\n",
    "        \n",
    "    # To get you started off, we'll pad the training input to 60 words to make it a square matrix.\n",
    "    train_mat = np.asarray([pad_to_length(np.array(ex.indexed_words), hp.seq_max_len) for ex in train_exs])\n",
    "    # Also store the actual sequence lengths.\n",
    "    train_seq_lens = np.array([len(ex.indexed_words) for ex in train_exs])\n",
    "    \n",
    "    # Training input reversed, useful is using bidirectional LSTM.\n",
    "    train_mat_rev = np.asarray([pad_to_length(np.array(ex.get_indexed_words_reversed()), hp.seq_max_len) for ex in train_exs])\n",
    "\n",
    "    # Extract labels.\n",
    "    train_labels_arr = np.array([ex.label for ex in train_exs])\n",
    "    targets = train_labels_arr\n",
    "    \n",
    "    # Extract embedding vectors.\n",
    "    embed_size = word_vectors.get_embedding_length()\n",
    "    embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "    \n",
    "    # Create RNN model.\n",
    "    rnnModel = RNNet(hp.lstm_size, hp.hidden_size, hp.lstm_layers, hp.drop_out,\n",
    "                     class_num, word_vectors, \n",
    "                     use_average, bidirectional,\n",
    "                     use_gpu =use_gpu)\n",
    "    \n",
    "    # If GPU is available, then run experiments on GPU\n",
    "    if use_gpu:\n",
    "        rnnModel.cuda()\n",
    "    \n",
    "    # Specify optimizer.\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, rnnModel.parameters()), \n",
    "                           lr = 5e-3, weight_decay  =5e-3, betas = (0.9, 0.9))\n",
    "    \n",
    "    # Define loss function: Binary Cross Entropy loss for logistic regression (binary classification).\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    \n",
    "    # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "    x = np.zeros((train_size, hp.seq_max_len, embed_size))\n",
    "    x_rev = np.zeros((train_size, hp.seq_max_len, embed_size))\n",
    "    for i in range(train_size):\n",
    "        x[i] = embeddings_vec[train_mat[i].astype(int)]\n",
    "        x_rev[i] = embeddings_vec[train_mat_rev[i].astype(int)]\n",
    "    \n",
    "    # Train the RNN model, gradient descent loop over minibatches.\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        rnnModel.train()\n",
    "        \n",
    "        ex_idxs = [i for i in range(train_size)]\n",
    "        random.shuffle(ex_idxs)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        start = 0\n",
    "        while start < train_size:\n",
    "            end = min(start + hp.batch_size, train_size)\n",
    "            \n",
    "            # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "            x_batch = form_input(x[ex_idxs[start:end]]).type(float_type)\n",
    "            x_batch_rev = form_input(x_rev[ex_idxs[start:end]]).type(float_type)\n",
    "            y_batch = form_input(targets[ex_idxs[start:end]]).type(float_type)\n",
    "            seq_lens_batch = train_seq_lens[ex_idxs[start:end]]\n",
    "            \n",
    "            # Compute output probabilities over all examples in minibatch.\n",
    "            probs = rnnModel(x_batch, x_batch_rev, seq_lens_batch).flatten()\n",
    "            \n",
    "            # Compute loss over all examples in minibatch.\n",
    "            loss = criterion(probs, y_batch)\n",
    "            total_loss += loss.data\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            start = end\n",
    "            \n",
    "        #print(\"Loss on epoch %i: %f\" % (epoch, total_loss))\n",
    "        \n",
    "        # Print accuracy on training and development data.\n",
    "        if epoch % 10 == 0:\n",
    "            acc = eval_model(rnnModel, train_exs, embeddings_vec, hp.seq_max_len)\n",
    "            print('Epoch', epoch, ': Accuracy on training set:', acc)\n",
    "            acc = eval_model(rnnModel, dev_exs, embeddings_vec, hp.seq_max_len)\n",
    "            print('Epoch', epoch, ': Accuracy on development set:', acc)\n",
    "            \n",
    "    # Evaluate model on the training dataset.\n",
    "    acc = eval_model(rnnModel, train_exs, embeddings_vec, hp.seq_max_len)\n",
    "    print('Accuracy on training set:', acc)\n",
    "    \n",
    "    # Evaluate model on the development dataset.\n",
    "    acc = eval_model(rnnModel, dev_exs, embeddings_vec, hp.seq_max_len)\n",
    "    print('Accuracy on develpment set:', acc)\n",
    "    \n",
    "    return rnnModel, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the testing (evaluation) procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on test examples and return predicted labels or accuracy.\n",
    "def eval_model(model, exs, embeddings_vec, seq_max_len, pred_only = False):\n",
    "    # Put model in evaluation mode.\n",
    "    model.eval()\n",
    "    \n",
    "    # Extract size pf word embedding.\n",
    "    embed_size = len(embeddings_vec[0])\n",
    "    \n",
    "    # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "    exs_mat = np.asarray([pad_to_length(np.array(ex.indexed_words), seq_max_len) for ex in exs])\n",
    "    exs_mat_rev = np.asarray([pad_to_length(np.array(ex.get_indexed_words_reversed()), seq_max_len) for ex in exs])\n",
    "    exs_seq_lens = np.array([len(ex.indexed_words) for ex in exs])\n",
    "    \n",
    "    # Get embeddings of words for forward and reverse sentence: (num_ex * seq_max_len * embedding_size)\n",
    "    x = np.zeros((len(exs), seq_max_len, embed_size))\n",
    "    x_rev = np.zeros((len(exs), seq_max_len, embed_size))\n",
    "    for i,ex in enumerate(exs):\n",
    "        x[i] = embeddings_vec[exs_mat[i].astype(int)]\n",
    "        x_rev[i] = embeddings_vec[exs_mat_rev[i].astype(int)]\n",
    "        \n",
    "    x = form_input(x)\n",
    "    x_rev = form_input(x_rev)\n",
    "    \n",
    "    # Run the model on the test examples.\n",
    "    preds = model(x, x_rev, exs_seq_lens).cpu().detach().numpy().flatten()\n",
    "    preds[preds >= 0.5] = 1\n",
    "    preds[preds < 0.5] = 0\n",
    "    \n",
    "    if pred_only == True:\n",
    "        return preds\n",
    "    else:\n",
    "        targets = np.array([ex.label for ex in exs])\n",
    "        return np.mean(preds == targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental evaluations on the Rotten Tomatoes dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, code for reading the examples and the corresponding GloVe word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in 30135 vectors of size 300\n",
      "8530 / 1066 / 1066 train / dev / test examples\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "word_vecs_path = '../data/glove.6B.300d-relativized.txt'\n",
    "\n",
    "train_path = '../data/rt/train.txt'\n",
    "dev_path = '../data/rt/dev.txt'\n",
    "blind_test_path = '../data/rt/test-blind.txt'\n",
    "test_output_path = 'test-blind.output.txt'\n",
    "\n",
    "word_vectors = read_word_embeddings(word_vecs_path)\n",
    "word_indexer = word_vectors.word_indexer\n",
    "\n",
    "train_exs = read_and_index_sentiment_examples(train_path, word_indexer)\n",
    "dev_exs = read_and_index_sentiment_examples(dev_path, word_indexer)\n",
    "test_exs = read_and_index_sentiment_examples(blind_test_path, word_indexer)\n",
    "\n",
    "print(repr(len(train_exs)) + \" / \" + \n",
    "      repr(len(dev_exs)) + \" / \" + \n",
    "      repr(len(test_exs)) + \" train / dev / test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use only the last state from one LSTM\n",
    "\n",
    "Evaluate One LSTM + fully connected network, use the last hidden state of LSTM. If the evaluation takes more than 1 hour on your computer, try reducing `lstm_size`, `hidden_size`, `batch_size` and even `num_epochs`.\n",
    "\n",
    "Our accuracy on development data is 75.98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.283130\n",
      "Epoch 0 : Accuracy on training set: 0.570926143024619\n",
      "Epoch 0 : Accuracy on development set: 0.6069418386491557\n",
      "Loss on epoch 1: 5.923219\n",
      "Loss on epoch 2: 5.186598\n",
      "Loss on epoch 3: 4.857446\n",
      "Loss on epoch 4: 4.525766\n",
      "Loss on epoch 5: 4.393498\n",
      "Loss on epoch 6: 4.267760\n",
      "Loss on epoch 7: 4.251276\n",
      "Loss on epoch 8: 4.342799\n",
      "Loss on epoch 9: 4.417922\n",
      "Loss on epoch 10: 4.228085\n",
      "Epoch 10 : Accuracy on training set: 0.77010550996483\n",
      "Epoch 10 : Accuracy on development set: 0.7373358348968105\n",
      "Loss on epoch 11: 4.186512\n",
      "Loss on epoch 12: 4.135224\n",
      "Loss on epoch 13: 4.117254\n",
      "Loss on epoch 14: 4.037282\n",
      "Loss on epoch 15: 4.025893\n",
      "Loss on epoch 16: 4.083275\n",
      "Loss on epoch 17: 3.935246\n",
      "Loss on epoch 18: 4.013088\n",
      "Loss on epoch 19: 3.892334\n",
      "Loss on epoch 20: 3.933729\n",
      "Epoch 20 : Accuracy on training set: 0.8029308323563892\n",
      "Epoch 20 : Accuracy on development set: 0.7495309568480301\n",
      "Loss on epoch 21: 3.904710\n",
      "Loss on epoch 22: 3.764803\n",
      "Loss on epoch 23: 3.747146\n",
      "Loss on epoch 24: 3.748110\n",
      "Loss on epoch 25: 3.761180\n",
      "Loss on epoch 26: 3.781156\n",
      "Loss on epoch 27: 3.618795\n",
      "Loss on epoch 28: 3.715480\n",
      "Loss on epoch 29: 3.710653\n",
      "Loss on epoch 30: 3.715240\n",
      "Epoch 30 : Accuracy on training set: 0.8153575615474795\n",
      "Epoch 30 : Accuracy on development set: 0.7570356472795498\n",
      "Loss on epoch 31: 3.677207\n",
      "Loss on epoch 32: 3.626993\n",
      "Loss on epoch 33: 3.631138\n",
      "Loss on epoch 34: 3.602581\n",
      "Loss on epoch 35: 3.569587\n",
      "Loss on epoch 36: 3.441028\n",
      "Loss on epoch 37: 3.374128\n",
      "Loss on epoch 38: 3.576374\n",
      "Loss on epoch 39: 3.516654\n",
      "Loss on epoch 40: 3.435211\n",
      "Epoch 40 : Accuracy on training set: 0.8339976553341149\n",
      "Epoch 40 : Accuracy on development set: 0.7673545966228893\n",
      "Loss on epoch 41: 3.310316\n",
      "Loss on epoch 42: 3.292491\n",
      "Loss on epoch 43: 3.384271\n",
      "Loss on epoch 44: 3.202982\n",
      "Loss on epoch 45: 3.242172\n",
      "Loss on epoch 46: 3.146991\n",
      "Loss on epoch 47: 3.119252\n",
      "Loss on epoch 48: 3.268133\n",
      "Loss on epoch 49: 3.043790\n",
      "Accuracy on training set: 0.8636576787807737\n",
      "Accuracy on develpment set: 0.7598499061913696\n",
      "Prediction written to file for Rotten Tomatoes dataset.\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = False\n",
    "bidirectional = False\n",
    "\n",
    "# Train RNN model.\n",
    "model1, acc = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the average of all states from one LSTM\n",
    "\n",
    "Evaluate One LSTM + fully connected network, use average of all states of the LSTM.\n",
    "\n",
    "Our accuracy on development data is 77.67%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.172609\n",
      "Epoch 0 : Accuracy on training set: 0.6438452520515826\n",
      "Epoch 0 : Accuracy on development set: 0.6388367729831145\n",
      "Loss on epoch 1: 5.432390\n",
      "Loss on epoch 2: 4.802831\n",
      "Loss on epoch 3: 4.599774\n",
      "Loss on epoch 4: 4.554598\n",
      "Loss on epoch 5: 4.353107\n",
      "Loss on epoch 6: 4.304817\n",
      "Loss on epoch 7: 4.248731\n",
      "Loss on epoch 8: 4.252585\n",
      "Loss on epoch 9: 4.265039\n",
      "Loss on epoch 10: 4.294543\n",
      "Epoch 10 : Accuracy on training set: 0.7772567409144197\n",
      "Epoch 10 : Accuracy on development set: 0.7598499061913696\n",
      "Loss on epoch 11: 4.186703\n",
      "Loss on epoch 12: 4.147440\n",
      "Loss on epoch 13: 4.139059\n",
      "Loss on epoch 14: 4.073933\n",
      "Loss on epoch 15: 4.085024\n",
      "Loss on epoch 16: 4.074733\n",
      "Loss on epoch 17: 4.006093\n",
      "Loss on epoch 18: 4.007493\n",
      "Loss on epoch 19: 3.953899\n",
      "Loss on epoch 20: 4.020343\n",
      "Epoch 20 : Accuracy on training set: 0.7982415005861665\n",
      "Epoch 20 : Accuracy on development set: 0.7570356472795498\n",
      "Loss on epoch 21: 3.892198\n",
      "Loss on epoch 22: 3.799976\n",
      "Loss on epoch 23: 3.829026\n",
      "Loss on epoch 24: 3.900204\n",
      "Loss on epoch 25: 3.898228\n",
      "Loss on epoch 26: 3.802075\n",
      "Loss on epoch 27: 3.711840\n",
      "Loss on epoch 28: 3.691252\n",
      "Loss on epoch 29: 3.610823\n",
      "Loss on epoch 30: 3.687838\n",
      "Epoch 30 : Accuracy on training set: 0.8109026963657678\n",
      "Epoch 30 : Accuracy on development set: 0.7664165103189493\n",
      "Loss on epoch 31: 3.681960\n",
      "Loss on epoch 32: 3.629795\n",
      "Loss on epoch 33: 3.588655\n",
      "Loss on epoch 34: 3.552782\n",
      "Loss on epoch 35: 3.464217\n",
      "Loss on epoch 36: 3.437990\n",
      "Loss on epoch 37: 3.495921\n",
      "Loss on epoch 38: 3.438723\n",
      "Loss on epoch 39: 3.468986\n",
      "Loss on epoch 40: 3.463244\n",
      "Epoch 40 : Accuracy on training set: 0.8263774912075029\n",
      "Epoch 40 : Accuracy on development set: 0.7495309568480301\n",
      "Loss on epoch 41: 3.334251\n",
      "Loss on epoch 42: 3.324419\n",
      "Loss on epoch 43: 3.448844\n",
      "Loss on epoch 44: 3.532736\n",
      "Loss on epoch 45: 3.443945\n",
      "Loss on epoch 46: 3.350998\n",
      "Loss on epoch 47: 3.241784\n",
      "Loss on epoch 48: 3.203223\n",
      "Loss on epoch 49: 3.180725\n",
      "Accuracy on training set: 0.8506447831184056\n",
      "Accuracy on develpment set: 0.776735459662289\n",
      "Prediction written to file for Rotten Tomatoes dataset using avg states from one LSTM.\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = True\n",
    "bidirectional = False\n",
    "\n",
    "# Train RNN model.\n",
    "model_lstm, acc = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model_lstm, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset using avg states from one LSTM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a bidirectional LSTM, concatenate last states\n",
    "\n",
    "Evaluate Two LSTMs (bidirectional) + fully connected network, concatenate their last states.\n",
    "\n",
    "Our accuracy on development data is 76.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.466570\n",
      "Epoch 0 : Accuracy on training set: 0.5389214536928487\n",
      "Epoch 0 : Accuracy on development set: 0.5562851782363978\n",
      "Loss on epoch 1: 6.128993\n",
      "Loss on epoch 2: 5.631390\n",
      "Loss on epoch 3: 4.830518\n",
      "Loss on epoch 4: 4.612877\n",
      "Loss on epoch 5: 4.397141\n",
      "Loss on epoch 6: 4.291522\n",
      "Loss on epoch 7: 4.167148\n",
      "Loss on epoch 8: 4.128596\n",
      "Loss on epoch 9: 4.102229\n",
      "Loss on epoch 10: 4.053901\n",
      "Epoch 10 : Accuracy on training set: 0.7955451348182884\n",
      "Epoch 10 : Accuracy on development set: 0.7617260787992496\n",
      "Loss on epoch 11: 3.940227\n",
      "Loss on epoch 12: 3.964778\n",
      "Loss on epoch 13: 4.016195\n",
      "Loss on epoch 14: 3.857199\n",
      "Loss on epoch 15: 3.837269\n",
      "Loss on epoch 16: 3.784451\n",
      "Loss on epoch 17: 3.764827\n",
      "Loss on epoch 18: 3.741179\n",
      "Loss on epoch 19: 3.598709\n",
      "Loss on epoch 20: 3.729750\n",
      "Epoch 20 : Accuracy on training set: 0.8161781946072685\n",
      "Epoch 20 : Accuracy on development set: 0.7551594746716698\n",
      "Loss on epoch 21: 3.549821\n",
      "Loss on epoch 22: 3.496562\n",
      "Loss on epoch 23: 3.502106\n",
      "Loss on epoch 24: 3.424518\n",
      "Loss on epoch 25: 3.459775\n",
      "Loss on epoch 26: 3.402848\n",
      "Loss on epoch 27: 3.267081\n",
      "Loss on epoch 28: 3.322020\n",
      "Loss on epoch 29: 3.224525\n",
      "Loss on epoch 30: 3.175074\n",
      "Epoch 30 : Accuracy on training set: 0.8518171160609613\n",
      "Epoch 30 : Accuracy on development set: 0.7607879924953096\n",
      "Loss on epoch 31: 3.194552\n",
      "Loss on epoch 32: 3.106477\n",
      "Loss on epoch 33: 3.085093\n",
      "Loss on epoch 34: 3.031401\n",
      "Loss on epoch 35: 3.032471\n",
      "Loss on epoch 36: 2.917463\n",
      "Loss on epoch 37: 2.975662\n",
      "Loss on epoch 38: 3.032947\n",
      "Loss on epoch 39: 2.908648\n",
      "Loss on epoch 40: 2.821387\n",
      "Epoch 40 : Accuracy on training set: 0.8718640093786636\n",
      "Epoch 40 : Accuracy on development set: 0.7439024390243902\n",
      "Loss on epoch 41: 2.798460\n",
      "Loss on epoch 42: 2.710805\n",
      "Loss on epoch 43: 2.742616\n",
      "Loss on epoch 44: 2.607796\n",
      "Loss on epoch 45: 2.616596\n",
      "Loss on epoch 46: 2.515622\n",
      "Loss on epoch 47: 2.713064\n",
      "Loss on epoch 48: 2.630630\n",
      "Loss on epoch 49: 2.457107\n",
      "Accuracy on training set: 0.8955451348182883\n",
      "Accuracy on develpment set: 0.7682926829268293\n",
      "Prediction written to file for Rotten Tomatoes dataset using bidirectional LSTM.\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "np.random.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "## YOUR CODE HERE\n",
    "hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                 hidden_size = 50, # hidden size of fully-connected layer\n",
    "                 lstm_layers = 1, # layers in lstm\n",
    "                 drop_out = 0.5, # dropout rate\n",
    "                 num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                 batch_size = 1024, # examples in a minibatch\n",
    "                 seq_max_len = 60) # maximum length of an example sequence\n",
    "use_average = False\n",
    "bidirectional = True\n",
    "\n",
    "# Train RNN model.\n",
    "model_lstm, acc = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "# Generate RNN model predictions for test set.\n",
    "embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "test_exs_predicted = eval_model(model_lstm, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "# Write the test set output\n",
    "for i, ex in enumerate(test_exs):\n",
    "    ex.label = int(test_exs_predicted[i])\n",
    "write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "print(\"Prediction written to file for Rotten Tomatoes dataset using bidirectional LSTM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a bidirectional LSTM, concatenate the averages of their states\n",
    "\n",
    "Evaluate Two LSTMs (bidirectional) + fully connected network, concatenate the averages of their states.\n",
    "\n",
    "Our accuracy on development data is 77.39%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on epoch 0: 6.466570\n",
      "Epoch 0 : Accuracy on training set: 0.5389214536928487\n",
      "Epoch 0 : Accuracy on development set: 0.5562851782363978\n",
      "Loss on epoch 1: 6.128993\n",
      "Loss on epoch 2: 5.631390\n",
      "Loss on epoch 3: 4.830518\n",
      "Loss on epoch 4: 4.612877\n",
      "Loss on epoch 5: 4.397141\n",
      "Loss on epoch 6: 4.291522\n",
      "Loss on epoch 7: 4.167148\n",
      "Loss on epoch 8: 4.128596\n",
      "Loss on epoch 9: 4.102229\n",
      "Loss on epoch 10: 4.053901\n",
      "Epoch 10 : Accuracy on training set: 0.7955451348182884\n",
      "Epoch 10 : Accuracy on development set: 0.7617260787992496\n",
      "Loss on epoch 11: 3.940227\n",
      "Loss on epoch 12: 3.964778\n",
      "Loss on epoch 13: 4.016195\n",
      "Loss on epoch 14: 3.857199\n",
      "Loss on epoch 15: 3.837269\n",
      "Loss on epoch 16: 3.784451\n",
      "Loss on epoch 17: 3.764827\n",
      "Loss on epoch 18: 3.741179\n",
      "Loss on epoch 19: 3.598709\n",
      "Loss on epoch 20: 3.729750\n",
      "Epoch 20 : Accuracy on training set: 0.8161781946072685\n",
      "Epoch 20 : Accuracy on development set: 0.7551594746716698\n",
      "Loss on epoch 21: 3.549821\n",
      "Loss on epoch 22: 3.496562\n",
      "Loss on epoch 23: 3.502106\n",
      "Loss on epoch 24: 3.424518\n",
      "Loss on epoch 25: 3.459775\n",
      "Loss on epoch 26: 3.402848\n",
      "Loss on epoch 27: 3.267081\n",
      "Loss on epoch 28: 3.322020\n",
      "Loss on epoch 29: 3.224525\n",
      "Loss on epoch 30: 3.175074\n",
      "Epoch 30 : Accuracy on training set: 0.8518171160609613\n",
      "Epoch 30 : Accuracy on development set: 0.7607879924953096\n",
      "Loss on epoch 31: 3.194552\n",
      "Loss on epoch 32: 3.106477\n",
      "Loss on epoch 33: 3.085093\n",
      "Loss on epoch 34: 3.031401\n",
      "Loss on epoch 35: 3.032471\n",
      "Loss on epoch 36: 2.917463\n",
      "Loss on epoch 37: 2.975662\n",
      "Loss on epoch 38: 3.032947\n",
      "Loss on epoch 39: 2.908648\n",
      "Loss on epoch 40: 2.821387\n",
      "Epoch 40 : Accuracy on training set: 0.8718640093786636\n",
      "Epoch 40 : Accuracy on development set: 0.7439024390243902\n",
      "Loss on epoch 41: 2.798460\n",
      "Loss on epoch 42: 2.710805\n",
      "Loss on epoch 43: 2.742616\n",
      "Loss on epoch 44: 2.607796\n",
      "Loss on epoch 45: 2.616596\n",
      "Loss on epoch 46: 2.515622\n",
      "Loss on epoch 47: 2.713064\n",
      "Loss on epoch 48: 2.630630\n",
      "Loss on epoch 49: 2.457107\n",
      "Accuracy on training set: 0.8955451348182883\n",
      "Accuracy on develpment set: 0.7682926829268293\n",
      "Prediction written to file for Rotten Tomatoes dataset using bidirectional LSTM and concatinating the average.\n"
     ]
    }
   ],
   "source": [
    "def modell_4():\n",
    "    random.seed(1)\n",
    "    np.random.seed(1)\n",
    "    torch.manual_seed(1)\n",
    "\n",
    "    ## YOUR CODE HERE\n",
    "    hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                     hidden_size = 50, # hidden size of fully-connected layer\n",
    "                     lstm_layers = 1, # layers in lstm\n",
    "                     drop_out = 0.5, # dropout rate\n",
    "                     num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                     batch_size = 1024, # examples in a minibatch\n",
    "                     seq_max_len = 60) # maximum length of an example sequence\n",
    "    use_average = True\n",
    "    bidirectional = True\n",
    "\n",
    "    # Train RNN model.\n",
    "    model_lstm, acc = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "    # Generate RNN model predictions for test set.\n",
    "    embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "    test_exs_predicted = eval_model(model_lstm, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "    # Write the test set output\n",
    "    for i, ex in enumerate(test_exs):\n",
    "        ex.label = int(test_exs_predicted[i])\n",
    "    write_sentiment_examples(test_exs, test_output_path, word_indexer)\n",
    "\n",
    "    print(\"Prediction written to file for Rotten Tomatoes dataset using bidirectional LSTM and concatinating the average.\")\n",
    "    \n",
    "    \n",
    "modell_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5111] Average performance and standard deviation\n",
    "\n",
    "The NN performance can vary depending on the random initialization of its parameters. Train and evaluate each model 10 times, from different random initializations (10 different seeds). Average the accuracy over the 10 runs and compare the performance of the 4 models on the Rotten Tomatoes dataset. Report in your analysis the average and standard deviation for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean, stdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "\n",
    "\n",
    "def avg_performance():\n",
    "    accuracy1 = []\n",
    "    accuracy2 = []\n",
    "    accuracy3 = []\n",
    "    accuracy4 = []\n",
    "\n",
    "\n",
    "    seeds = [0,13,27,3,42,55,69,7,81,97]\n",
    "    hp = HyperParams(lstm_size = 50, # hidden units in lstm\n",
    "                     hidden_size = 50, # hidden size of fully-connected layer\n",
    "                     lstm_layers = 1, # layers in lstm\n",
    "                     drop_out = 0.5, # dropout rate\n",
    "                     num_epochs = 50, # number of epochs for SGD-based procedure\n",
    "                     batch_size = 1024, # examples in a minibatch\n",
    "                     seq_max_len = 60) # maximum length of an example sequence\n",
    "\n",
    "    for s in seeds:\n",
    "\n",
    "        random.seed(s)\n",
    "        np.random.seed(s)\n",
    "        torch.manual_seed(s)\n",
    "\n",
    "        #not using avg and bidirectional\n",
    "        use_average = False\n",
    "        bidirectional = False\n",
    "\n",
    "        # Train RNN model.\n",
    "        model1, acc1 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "        # Generate RNN model predictions for test set.\n",
    "        embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "        test_exs_predicted = eval_model(model1, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "        print(\"Prediction written to file for Rotten Tomatoes dataset.\")\n",
    "        accuracy1.append(acc1)\n",
    "\n",
    "        #using average\n",
    "        use_average1 = True\n",
    "        bidirectional1 = False\n",
    "        # Train RNN model.\n",
    "        modell, acc2 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "        # Generate RNN model predictions for test set.\n",
    "        embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "        test_exs_predicted = eval_model(modell, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "        print(\"Prediction written to file for Rotten Tomatoes dataset average.\")\n",
    "        accuracy2.append(acc2)\n",
    "\n",
    "        #using bidirectional\n",
    "\n",
    "        use_average1 = False\n",
    "        bidirectional1 = True\n",
    "        # Train RNN model.\n",
    "        modell, acc3 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "        # Generate RNN model predictions for test set.\n",
    "        embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "        test_exs_predicted = eval_model(modell, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "        print(\"Prediction written to file for Rotten Tomatoes dataset bidirectional LSTM.\")\n",
    "        accuracy3.append(acc3)\n",
    "\n",
    "        #using bidirectional and average\n",
    "        use_average1 = True\n",
    "        bidirectional1 = True\n",
    "        # Train RNN model.\n",
    "        modell, acc4 = train_model(hp, train_exs, dev_exs, test_exs, word_vectors, use_average, bidirectional)\n",
    "\n",
    "        # Generate RNN model predictions for test set.\n",
    "        embeddings_vec = np.array(word_vectors.vectors).astype(float)\n",
    "        test_exs_predicted = eval_model(modell, test_exs, embeddings_vec, hp.seq_max_len, pred_only = True)\n",
    "\n",
    "        print(\"Prediction written to file for Rotten Tomatoes dataset bidirectional LSTM and concatinating\")\n",
    "        accuracy4.append(acc4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    print(f'average of the model 1 = {mean(accuracy1)} and std deviation = {stdev(accuracy1)}')\n",
    "    print(f'average of the model 2 = {mean(accuracy2)} and std deviation = {stdev(accuracy2)}')\n",
    "    print(f'average of the model 3 = {mean(accuracy3)} and std deviation = {stdev(accuracy3)}')\n",
    "    print(f'average of the model 4 = {mean(accuracy4)} and std deviation = {stdev(accuracy4)}')\n",
    "    \n",
    "avg_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimental evaluations on the IMDB dataset.\n",
    "\n",
    "Run the same 4 evaluations on the IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_path = '../data/imdb/train.txt'\n",
    "dev_path = '../data/imdb/dev.txt'\n",
    "test_path = '../data/imdb/test.txt'\n",
    "\n",
    "test_output_path = 'test-imdb.output.txt'\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "word_vectors = read_word_embeddings(word_vecs_path)\n",
    "word_indexer = word_vectors.word_indexer\n",
    "\n",
    "#train_exs = read_and_index_sentiment_examples(train_path, word_indexer)\n",
    "dev_exs = read_and_index_sentiment_examples(dev_path, word_indexer)\n",
    "test_exs = read_and_index_sentiment_examples(test_path, word_indexer)\n",
    "\n",
    "print(repr(len(train_exs)) + \" / \" + \n",
    "      repr(len(dev_exs)) + \" / \" + \n",
    "      repr(len(test_exs)) + \" train / dev / test examples\")\n",
    "\n",
    "avg_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [5111] Cross-domain performance\n",
    "\n",
    "Compare the performance of the Bidirectional LSTM with state averaging on the IMDB test set in two scenarios:\n",
    "\n",
    "1. The model is trained on the IMDB training data.\n",
    "\n",
    "2. The model is trained on the Rotten Tomatoes data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "#1\n",
    "train_path = '../data/imdb/train.txt'\n",
    "dev_path = '../data/imdb/dev.txt''\n",
    "test_path = '../data/imdb/test.txt'\n",
    "\n",
    "test_output_path = 'test-imdb_1.output.txt'\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "word_vectors = read_word_embeddings(word_vecs_path)\n",
    "word_indexer = word_vectors.word_indexer\n",
    "\n",
    "#train_exs = read_and_index_sentiment_examples(train_path, word_indexer)\n",
    "dev_exs = read_and_index_sentiment_examples(dev_path, word_indexer)\n",
    "test_exs = read_and_index_sentiment_examples(test_path, word_indexer)\n",
    "\n",
    "print(repr(len(train_exs)) + \" / \" + \n",
    "      repr(len(dev_exs)) + \" / \" + \n",
    "      repr(len(test_exs)) + \" train / dev / test examples\")\n",
    "\n",
    "modell_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "#2\n",
    "train_path = '../data/rt/train.txt'\n",
    "dev_path = '../data/imdb/dev.txt''\n",
    "test_path = '../data/imdb/test.txt'\n",
    "\n",
    "test_output_path = 'test-imdb_2.output.txt'\n",
    "\n",
    "## YOUR CODE HERE\n",
    "\n",
    "word_vectors = read_word_embeddings(word_vecs_path)\n",
    "word_indexer = word_vectors.word_indexer\n",
    "\n",
    "#train_exs = read_and_index_sentiment_examples(train_path, word_indexer)\n",
    "dev_exs = read_and_index_sentiment_examples(dev_path, word_indexer)\n",
    "test_exs = read_and_index_sentiment_examples(test_path, word_indexer)\n",
    "\n",
    "print(repr(len(train_exs)) + \" / \" + \n",
    "      repr(len(dev_exs)) + \" / \" + \n",
    "      repr(len(test_exs)) + \" train / dev / test examples\")\n",
    "\n",
    "modell_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "\n",
    "Anything extra goes here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "\n",
    "Include an analysis of the results that you obtained in the experiments above. Also compare with the sentiment classification performance from previous assignments and explain the difference in accuracy. Show the results using table(s).\n",
    "<ul>\n",
    "    <li>The accuracy of the models while testing the IMDB dataset after training on first the IMDB daatset and then the rotten tomato dataset</li>\n",
    "    <li>The accuracy of the model 4 i.e when we use the bidirectional LSTM and concatinating the average is the maximum in most cases as we can see in the first part of the assignment</li>\n",
    "    <li>The logistic regression model performed better than the LSTM as we can see from the previous assignmnets</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
