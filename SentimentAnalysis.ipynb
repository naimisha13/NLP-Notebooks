{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes for Sentiment Analysis \n",
    "\n",
    "This assignment is comprised of two parts:\n",
    "\n",
    "1. **Theory**: Solve the Naive Bayes exercises 4.1 and 4.2 from Chapter 4 in the J&M textbook. Reformulate NB to emphasize title words.\n",
    "2. **Implementation**: You will implement and experiment with various feature engineering techniques in the context of Naive Bayes models for Sentiment classification of movie reviews.\n",
    "\n",
    "We will use the NB model implemented in sklearn:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Naimisha Churi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"blue\"> Submission Instructions</font>\n",
    "\n",
    "1. Click the Save button at the top of the Jupyter Notebook.\n",
    "2. Please make sure to have entered your name above.\n",
    "3. Select Cell -> All Output -> Clear. This will clear all the outputs from all cells (but will keep the content of all cells). \n",
    "4. Select Cell -> Run All. This will run all the cells in order, and will take several minutes.\n",
    "5. Once you've rerun everything, select File -> Download as -> PDF via LaTeX and download a PDF version *SentimentAnalysis.pdf* showing the code and the output of all cells, and save it in the same folder that contains the notebook file *SentimentAnalysis.ipynb*.\n",
    "6. Look at the PDF file and make sure all your solutions are there, displayed correctly. The PDF is the only thing we will see when grading!\n",
    "7. Submit **both** your PDF and notebook on Canvas.\n",
    "8. Verify your Canvas submission contains the correct files by downloading it after posting it on Canvas.\n",
    "\n",
    "*Make sure that you format your solutions to theory questions to show equations properly. We will not grade solutions that are not properly formatted. Jupyter-notebook understands Latex, alternatively you can edit in Word using its Equation editor and submit the PDF as a separate file in Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: J&M Exercise 4.1 ##\n",
    "Your solution goes here ...\n",
    "<br>`Negative`<br>\n",
    "We are given the liklihoods of each word in a document.<br>\n",
    "The positive and negative classes are equally likely. Therefore the probability is 1/2 as we assume them to be exhaustive<br>\n",
    "P<sub>p</sub> = P(Document|Positive) = 0.5 * 0.09 * 0.07 * 0.29 * 0.04 * 0.08 = 0.00000292 <br>\n",
    "P<sub>n</sub> = P(Document|Negative) = 0.5 * 0.16 * 0.06 * 0.06 * 0.15 * 0.11 = 0.00000475 <br>\n",
    "\n",
    "Since P<sub>n</sub> > P<sub>p</sub>, <br>\n",
    "The document is classified as a Negative movie review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory: J&M Exercise 4.2 ##\n",
    "Your solution goes here ...<br>`Action genre`<br>\n",
    "Given classes: Comedy, Action<br>\n",
    "P(Comedy) = 2/5 = 0.4<br>\n",
    "P(Action) = 3/5 = 0.6<br>\n",
    "\n",
    "Given document: fast, couple, shoot, fly<br>\n",
    "\n",
    "Let's consider the action class,<br>\n",
    "P(w<sub>i</sub>|action) = (count(w<sub>i</sub>,Action)+1)/(Sum(count(w,Action))+|V|) <br>\n",
    "P(fast|Action) = (2+1)/(11+7) = 3/18 <br>\n",
    "P(couple|Action) = (0+1)/(11+7) = 1/18 <br>\n",
    "P(shoot|Action) = (4+1)/(11+7) = 5/18 <br>\n",
    "P(fly|Action) = (1+1)/(11+7) = 2/18 <br>\n",
    "P(Action|Sentence) = P(Action) x (P(w<sub>i</sub>|Action)) <br>\n",
    "P<sub>A</sub> = P(Action|Sentence) = (3/5)x(3x1x5x2/(18<sup>4</sup>)) = 0.000171 <br><br>\n",
    "P(w<sub>i</sub>|Comedy) = (count(w<sub>i</sub>,Comedy)+1)/(Sum(count(w,Comedy))+|V|) <br>\n",
    "P(fast|Comedy) = (1+1)/(9+7) = 2/16 <br>\n",
    "P(couple|Comedy) = (2+1)/(9+7) = 3/16 <br>\n",
    "P(shoot|Comedy) = (0+1)/(9+7) = 1/16 <br>\n",
    "P(fly|Comedy) = (1+1)/(9+7) = 2/16 <br>\n",
    "<br>\n",
    "Therefore, P(Comedy|Sentence) = P(Comedy) x P(w<sub>i</sub>|Comedy)) <br>\n",
    "P<sub>C</sub> = P(Comedy|Sentence) = (2/5)x(2x3x1x2/(16<sup>4</sup>)) = 0.00007324 <br>\n",
    "\n",
    "p̂ = argmax(P<sub>A</sub>,P<sub>C</sub>) = 0.000171 <br><br>\n",
    "Therefore, the class of the document is Action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory 5111: Title is $K$ times more important than Body\n",
    "*Mandatory for graduate students, optional for undergraduate students*\n",
    "\n",
    "The Naive Bayes algorithm for text categorization presented in class treats all sections of a document equally, ignoring the fact that words in the title are often more important than words in the text in determining the document category. Modify the Naive Bayes training algorithm to reflect that word occurrences in the title are $K$ times more important than word occurrences in the rest of the document for deciding the class, where $K$ is an input parameter. Describe the idea in English and include pseudocode, akin to the training pseudocode shown in class.\n",
    "\n",
    "Your solution goes here ...<br>\n",
    "There are 3 approaches I came up with in which we can approach this<br>\n",
    "<ol>\n",
    "  <li>We can just add $K$ to the frequency of the word that appears in the title and add $K$*|title words| to the total number of words</li>\n",
    "  <li>We can also multiply $K$ to the probability of all the words in the title set. The problem with this one is the </li>\n",
    "  <li>We can multiply $K$ to the frequency(n) of the word that appears in the title and add n*($K$ - 1) to the total number of words for each word in the title set</li>\n",
    "</ol> \n",
    "\n",
    "<br>\n",
    "Step 1: create a set title words for each class<br>\n",
    "Step 2: create the vocabulary with frequency of each word<br>\n",
    "Step 3: calculate the frequency of each word and if that word appears in the set of title words, multiply it with $K$<br>\n",
    "Step 4: for every word in title words set we add n*($K$ - 1) to the total number of words where n is the orignal frequency of the title word <br>\n",
    "Step 5: Calculate the parameters of Naïve Bayes' to calculate the proability of the unseen document, the usual working of Naïve Bayes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory Bonus: Maximum likelihood estimation\n",
    "\n",
    "\n",
    "The <a href=\"https://en.wikipedia.org/wiki/Poisson_distribution\">Poisson distribution</a> specifies the probability of observing *k* events in an interval,\n",
    "as follows:\n",
    "\n",
    "P(*k* events in interval) = $e^{-\\lambda}\\frac{\\lambda^k}{k!}$\n",
    "\n",
    "For example, $k$ can be the number of meteors greater than 1 meter diameter that strike Earth in a year, or the number of patients arriving in an emergency room between 10 and 11 pm.\n",
    "\n",
    "Suppose we observe $N$ samples $k_1$, $k_2$, ..., $k_N$ from this distribution (i.e. numbers of meteors that strike Earth over a period of N years). Derive the maximum likelihood estimate of the event rate $λ$.\n",
    "\n",
    "\n",
    "Your solution goes here ... #todo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From documents to feature vectors\n",
    "This section illustratess the prototypical components of machine learning pipeline for an NLP task, in this case document classification:\n",
    "\n",
    "1. Read document examples (train, devel, test) from files with a predefined format:\n",
    "    - assume one document per line, usign the format \"\\<label\\> \\<text\\>\".\n",
    "\n",
    "2. Tokenize each document:\n",
    "    - using a spaCy tokenizer.\n",
    "\n",
    "3. Feature extractors:\n",
    "    - so far, just words.\n",
    "\n",
    "4. Process each document into a feature vector:\n",
    "    - map document to a dictionary of feature names.\n",
    "    - map feature names to unique feature IDs.\n",
    "    - each document is a feature vector, where each feature ID is mapped to a feature value (e.g. word occurences)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "from scipy import sparse\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spaCy tokenizer.\n",
    "spacy_nlp = English()\n",
    "\n",
    "def spacy_tokenizer(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    \n",
    "    return [token.text for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples(filename):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(filename, mode = 'r', encoding = 'utf-8') as file:\n",
    "        for line in file:\n",
    "            [label, text] = line.rstrip().split(' ', maxsplit = 1)\n",
    "            X.append(text)\n",
    "            Y.append(label)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(tokens):\n",
    "    feats = {}\n",
    "    for word in tokens:\n",
    "        feat = 'WORD_%s' % word\n",
    "        if feat in feats:\n",
    "            feats[feat] +=1\n",
    "        else:\n",
    "            feats[feat] = 1\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(feats, new_feats):\n",
    "    for feat in new_feats:\n",
    "        if feat in feats:\n",
    "            feats[feat] += new_feats[feat]\n",
    "        else:\n",
    "            feats[feat] = new_feats[feat]\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function tokenizes the document, runs all the feature extractors on it and assembles the extracted features into a dictionary mapping feature names to feature values. It is important that feature names do not conflict with each other, i.e. different features should have different names. Each document will have its own dictionary of features and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2features(trainX, feature_functions, tokenizer):\n",
    "    examples = []\n",
    "    count = 0\n",
    "    for doc in trainX:\n",
    "        feats = {}\n",
    "\n",
    "        tokens = tokenizer(doc)\n",
    "        \n",
    "        for func in feature_functions:\n",
    "            add_features(feats, func(tokens))\n",
    "\n",
    "        examples.append(feats)\n",
    "        count +=1\n",
    "        \n",
    "        if count % 100 == 0:\n",
    "            print('Processed %d examples into features' % len(examples))\n",
    "    \n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts feature names to unique numerical IDs.\n",
    "\n",
    "def create_vocab(examples):\n",
    "    feature_vocab = {}\n",
    "    idx = 0\n",
    "    for example in examples:\n",
    "        for feat in example:\n",
    "            if feat not in feature_vocab:\n",
    "                feature_vocab[feat] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    return feature_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This helper function converts a set of examples from a dictionary of feature names to values representation\n",
    "# to a sparse representation of feature ids to values. This is important because almost all feature values will\n",
    "# be 0 for most documents and it would be wasteful to save all in memory.\n",
    "\n",
    "def features_to_ids(examples, feature_vocab):\n",
    "    new_examples = sparse.lil_matrix((len(examples), len(feature_vocab)))\n",
    "    for idx, example in enumerate(examples):\n",
    "        for feat in example:\n",
    "            if feat in feature_vocab:\n",
    "                new_examples[idx, feature_vocab[feat]] = example[feat]\n",
    "                \n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation pipeline for the Naive Bayes classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 28692\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.779\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "datapath = '../data'\n",
    "\n",
    "train_file = os.path.join(datapath, 'imdb_sentiment_train.txt')\n",
    "trainX, trainY = read_examples(train_file)\n",
    "\n",
    "dev_file = os.path.join(datapath, 'imdb_sentiment_dev.txt')\n",
    "devX, devY = read_examples(dev_file)\n",
    "\n",
    "# Specify features to use.\n",
    "features = [word_features]\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when using only alpha tokens. This can be done by changing the tokenizer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 25054\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.785\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer1(text):\n",
    "    tokens = spacy_nlp.tokenizer(text)\n",
    "    token_alpha = []\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Keep in the tokens list only those whose text is made up solely from letters.\n",
    "    for token in tokens:\n",
    "        if token.text.isalpha():\n",
    "            token_alpha.append(token.text)\n",
    "    \n",
    "    return token_alpha\n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but lowercase all tokens before using as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21703\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.782\n"
     ]
    }
   ],
   "source": [
    "def spacy_tokenizer2(text):\n",
    "    tokens = spacy_nlp.tokenizer(text.lower())\n",
    "    token_alpha = []\n",
    "    # YOUR CODE HERE\n",
    "    # Keep in the tokens list only those whose text is made up solely from letters.\n",
    "    # Return a list of lowercased token text.\n",
    "    for token in tokens:\n",
    "        if token.text.isalpha():\n",
    "            token_alpha.append(token.text)\n",
    "    \n",
    "    return token_alpha\n",
    "    \n",
    "\n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same as above, but lowercase only tokens that appear at the beginning of sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 24891\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.795\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = English()\n",
    "\n",
    "spacy_nlp.add_pipe(\"sentencizer\")    \n",
    "\n",
    "def spacy_tokenizer3(text):\n",
    "    #doc = spacy_nlp(text)\n",
    "    \n",
    "    tokens = []\n",
    "    sentences = text.split('\\n')\n",
    "    for sent in sentences:\n",
    "        if sent.strip():\n",
    "            s = spacy_nlp(sent)\n",
    "            for token in s:\n",
    "                if token.text.isalpha()and token.text.lower() not in tokens:\n",
    "                    if token.is_sent_start:\n",
    "                        tokens.append(token.text.lower())\n",
    "                    else:\n",
    "                        tokens.append(token.text)\n",
    "                        \n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "        \n",
    "#     for i in range(0,5):\n",
    "#         print(doc[i])\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Evaluate NB model.\n",
    "#spacy_tokenizer3(text)\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spacy_tokenizer2 (only alpha tokens, lowered all) and display the top 10 most frequent tokens in the vocabulary, as a list of tuples (token, frequency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21703\n",
      "('WORD_the', 1487)\n",
      "('WORD_a', 1461)\n",
      "('WORD_and', 1459)\n",
      "('WORD_of', 1418)\n",
      "('WORD_to', 1417)\n",
      "('WORD_is', 1349)\n",
      "('WORD_this', 1341)\n",
      "('WORD_in', 1313)\n",
      "('WORD_it', 1311)\n",
      "('WORD_that', 1208)\n",
      "('WORD_i', 1166)\n",
      "('WORD_for', 1076)\n",
      "('WORD_but', 1062)\n",
      "('WORD_with', 1061)\n",
      "('WORD_was', 986)\n",
      "('WORD_as', 960)\n",
      "('WORD_on', 944)\n",
      "('WORD_not', 907)\n",
      "('WORD_are', 884)\n",
      "('WORD_movie', 878)\n"
     ]
    }
   ],
   "source": [
    "# First, count token occurrences across all examples, where features are still strings. \n",
    "def create_feature_counts(examples):\n",
    "    feature_counts = {}\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    #tokens = spacy_nlp(examples)\n",
    "    for d in examples:\n",
    "        for token, count in d.items():\n",
    "            if token in feature_counts:\n",
    "                feature_counts[token] += 1\n",
    "            else:\n",
    "                feature_counts[token] = 1\n",
    "    return feature_counts\n",
    "\n",
    "# Create features for all training examples, compute feature counts \n",
    "def fcounts_from_train(trainX, feature_functions, tokenizer):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_counts))\n",
    "\n",
    "    return feature_counts\n",
    "\n",
    "# Return a list of the top K most frequent tokens in the vocabulary.\n",
    "def topK_tokens(vocab, k):\n",
    "    # YOUR CODE HERE\n",
    "    ranked = sorted(vocab.items(), reverse = True, key=lambda key: key[1])\n",
    "    return ranked[:k]\n",
    "\n",
    "vocab = fcounts_from_train(trainX, features, spacy_tokenizer2)\n",
    "stop_words = topK_tokens(vocab, 20)\n",
    "for item in stop_words:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when ignoring the top 20 stop words. Use spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21683\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.796\n"
     ]
    }
   ],
   "source": [
    "# Evaluation pipeline for the Naive Bayes classifier.\n",
    "\n",
    "def train_and_test(trainX, trainY, devX, devY, feature_functions, tokenizer, n):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    #i = 0\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    stop_words = topK_tokens(vocab, n)\n",
    "\n",
    "    # Remove from each example features that appear in the stop words list.\n",
    "    # YOUR CODE HERE.\n",
    "#     for word, count in stop_words:\n",
    "#         del feature_counts[word]\n",
    "    \n",
    "    for token in trainX_feat:\n",
    "        #print('lalala')\n",
    "        for word, count in stop_words:\n",
    "            #print(word)\n",
    "            if word in token:\n",
    "                #print(word)\n",
    "                #print('lalala1')\n",
    "                del token[word]\n",
    "\n",
    "    #for i in range\n",
    "        #i += 1\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "    \n",
    "# Evaluate NB model.\n",
    "train_and_test(trainX, trainY, devX, devY, features, spacy_tokenizer2,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate NB model performance when ignoring words that appear less than 5 times. Use spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 4874\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.794\n"
     ]
    }
   ],
   "source": [
    "def bottomK_tokens(vocab, n):\n",
    "    # YOUR CODE HERE\n",
    "    ranked = {}\n",
    "    for word, count in vocab.items():\n",
    "        if count < n:\n",
    "            ranked[word] = count\n",
    "    \n",
    "    return ranked\n",
    "    \n",
    "def train_and_test1(trainX, trainY, devX, devY, feature_functions, tokenizer, n):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    \n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    #stop_words = lambda item for item in vocab.keys() if vocab[item] < n\n",
    "    stop_words = (feature_counts, n)\n",
    "    # Remove from each example features that appear in the stop words list.\n",
    "    # YOUR CODE HERE.\n",
    "#     for word, count in stop_words:\n",
    "#         del feature_counts[word]\n",
    "    i = 0\n",
    "    for token in trainX_feat:\n",
    "        for word,count in stop_words.items():\n",
    "            #print(count)\n",
    "            if word in token:\n",
    "                #print('lalala')\n",
    "                del trainX_feat[i][word]\n",
    "        i += 1\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "    \n",
    "# Evaluate NB model.\n",
    "train_and_test1(trainX, trainY, devX, devY, features, spacy_tokenizer2,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Multinomial Bayes\n",
    "*Mandatory for graduate students, optional for undergraduate students*\n",
    "\n",
    "Write code for transforming documents to features such that features are Boolean and only represent whether a word occurred in a document, as in the Binary Multinomial Naive Bayes discussed in class. Evaluate the Naive Bayes model with this feature representation, using spacy_tokenizer2 (only alpha tokens, lowered all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Vocabulary size: 21703\n",
      "Processed 100 examples into features\n",
      "Processed 200 examples into features\n",
      "Processed 300 examples into features\n",
      "Processed 400 examples into features\n",
      "Processed 500 examples into features\n",
      "Processed 600 examples into features\n",
      "Processed 700 examples into features\n",
      "Processed 800 examples into features\n",
      "Processed 900 examples into features\n",
      "Processed 1000 examples into features\n",
      "Processed 1100 examples into features\n",
      "Processed 1200 examples into features\n",
      "Processed 1300 examples into features\n",
      "Processed 1400 examples into features\n",
      "Processed 1500 examples into features\n",
      "Accuracy: 0.793\n"
     ]
    }
   ],
   "source": [
    "def binary_multinomial_bayes(vocab):\n",
    "    \n",
    "    return None\n",
    "\n",
    "def train_and_test2(trainX, trainY, devX, devY, feature_functions, tokenizer, n):\n",
    "    # Pre-process training documents. \n",
    "    trainX_feat = docs2features(trainX, feature_functions, tokenizer)\n",
    "    \n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_counts = create_feature_counts(trainX_feat)\n",
    "    #stop_words = lambda item for item in vocab.keys() if vocab[item] < n\n",
    "    stop_words = n_least_freq(vocab,n)\n",
    "    # Remove from each example features that appear in the stop words list.\n",
    "    # YOUR CODE HERE.\n",
    "#     for word, count in stop_words:\n",
    "#         del feature_counts[word]\n",
    "    i = 0\n",
    "    for token in trainX_feat:\n",
    "        for word in stop_words:\n",
    "            if word in trainX_feat[i]:\n",
    "                trainX_feat[i][word] = 1\n",
    "            else:\n",
    "                trainX_feat[i][word] = 0\n",
    "        i += 1\n",
    "    # Create vocabulary from features in training examples.\n",
    "    feature_vocab = create_vocab(trainX_feat)\n",
    "    print('Vocabulary size: %d' % len(feature_vocab))\n",
    "\n",
    "    trainX_ids = features_to_ids(trainX_feat, feature_vocab)\n",
    "    \n",
    "    # Train NB model.\n",
    "    nb_model = MultinomialNB(alpha = 1.0)\n",
    "    nb_model.fit(trainX_ids, trainY)\n",
    "    \n",
    "    # Pre-process test documents. \n",
    "    devX_feat = docs2features(devX, feature_functions, tokenizer)\n",
    "    devX_ids = features_to_ids(devX_feat, feature_vocab)\n",
    "    \n",
    "    # Test NB model.\n",
    "    print('Accuracy: %.3f' % nb_model.score(devX_ids, devY))\n",
    "    \n",
    "\n",
    "train_and_test2(trainX, trainY, devX, devY, features, spacy_tokenizer2,5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus points ##\n",
    "Anything extra goes here. For example, implement NB from scratch in a separate module nbayes.py and use it for the exercises above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis ##\n",
    "Include an analysis of the results that you obtained in the experiments above. Take advantage of the Jupyter Notebook markdown language, which can also process Latex and HTML, to format your report so that it looks professional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the Naïve Bayes' model increased when we ignored the top 20 stop words as it should. When we ignore the words that appear less than 5 times in the vocabulary, we get a higher frequency. The accuracy also increased which is strange because we are essentially ignoring the weightage of every word by ignoring the number of times it appears in the document. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
